{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb39525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, RobustScaler\n",
    "import stealing_verification.sort.mlp as mlp\n",
    "import os\n",
    "from torch.amp import autocast\n",
    "import pickle\n",
    "\n",
    "def split_train_test(vict_o, benign_o, scale=0.8, shuffle=False):\n",
    "    # split the train & test set for training clf\n",
    "    indices_vict = np.arange(len(vict_o))\n",
    "    indices_benign = np.arange(len(benign_o))\n",
    "\n",
    "    indices_train_vict_o = np.random.choice(indices_vict, int(len(vict_o)*scale), replace=False)\n",
    "    indices_test_vict_o = np.setdiff1d(indices_vict, indices_train_vict_o)\n",
    "\n",
    "    indices_train_benign_o = np.random.choice(indices_benign, int(len(benign_o) * scale), replace=False)\n",
    "    indices_test_benign_o = np.setdiff1d(indices_benign, indices_train_benign_o)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices_train_vict_o)\n",
    "        np.random.shuffle(indices_train_benign_o)\n",
    "\n",
    "    train_vict_o = []\n",
    "    test_vict_o = []\n",
    "    train_benign_o = []\n",
    "    test_benign_o = []\n",
    "\n",
    "    test_num = min(len(indices_test_benign_o), len(indices_test_vict_o))\n",
    "\n",
    "    for id in indices_train_vict_o:\n",
    "        train_vict_o.append(vict_o[id])\n",
    "\n",
    "    for id in indices_test_vict_o:\n",
    "        test_vict_o.append(vict_o[id])\n",
    "\n",
    "    for id in indices_train_benign_o:\n",
    "        train_benign_o.append(benign_o[id])\n",
    "\n",
    "    for id in indices_test_benign_o:\n",
    "        test_benign_o.append(benign_o[id])\n",
    "\n",
    "    test_vict_o = test_vict_o[:test_num]\n",
    "    test_benign_o = test_benign_o[:test_num]\n",
    "\n",
    "    return train_vict_o, test_vict_o, train_benign_o, test_benign_o\n",
    "\n",
    "def get_outputs_set(o_p, o_np, norm=False, scale=False, output_clf_dir=None):\n",
    "    o_trainset = np.vstack([o_p, o_np])\n",
    "    o_label = np.concatenate([np.ones(len(o_p)), np.zeros(len(o_np))])\n",
    "\n",
    "    if norm:\n",
    "        normalizer = Normalizer(norm='l2')\n",
    "        o_trainset = normalizer.transform(o_trainset)\n",
    "        if output_clf_dir is not None:\n",
    "            os.makedirs(output_clf_dir, exist_ok=True)\n",
    "            save_dir = os.path.join(output_clf_dir, 'normalizer.pkl')\n",
    "            with open(save_dir, 'wb') as f:\n",
    "                pickle.dump(normalizer, f)\n",
    "\n",
    "    if scale:\n",
    "        scaler = RobustScaler()\n",
    "        o_trainset = scaler.fit_transform(o_trainset)\n",
    "        if output_clf_dir is not None:\n",
    "            save_dir = os.path.join(output_clf_dir, 'scaler.pkl')\n",
    "            with open(save_dir, 'wb') as f:\n",
    "                pickle.dump(scaler, f)\n",
    "\n",
    "\n",
    "    return o_trainset, o_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430261db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load output of victim and benign\n"
     ]
    }
   ],
   "source": [
    "# load outputset\n",
    "seed = 42\n",
    "softmax = False\n",
    "norm = True\n",
    "scale = True\n",
    "output_clf_dir = './ckpt/clf/'\n",
    "mlp_epoch = 300\n",
    "\n",
    "vict_output = './outputset-ckpt/victim/benign_sample_train_logits_list.npy'\n",
    "poison_output = './outputset-ckpt/poisoned/benign_sample_train_logits_list.npy'\n",
    "benign_output = './outputset-ckpt/benign/benign_sample_train_logits_list.npy'\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "starttime = time.time()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('load output of victim and benign')\n",
    "\n",
    "vict_output = np.load(vict_output)\n",
    "poison_output = np.load(poison_output)\n",
    "benign_output = np.load(benign_output)\n",
    "\n",
    "if softmax:\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    vict_output = softmax(torch.from_numpy(np.array(vict_output))).numpy()\n",
    "    poison_output = softmax(torch.from_numpy(np.array(poison_output))).numpy()\n",
    "    benign_output = softmax(torch.from_numpy(np.array(benign_output))).numpy()\n",
    "\n",
    "vict_v = vict_output - poison_output\n",
    "benign_v = benign_output - poison_output\n",
    "\n",
    "# split train & test set for training clf\n",
    "train_vict_o, test_vict_o, train_benign_o, test_benign_o = split_train_test(vict_v, benign_v)\n",
    "\n",
    "outputs_trainset, outputs_trainlabel = get_outputs_set(train_vict_o, train_benign_o, norm=norm, scale=scale, output_clf_dir=output_clf_dir)\n",
    "\n",
    "outputs_testset, outputs_testlabel = get_outputs_set(test_vict_o, test_benign_o, norm=norm, scale=scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4da8d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train meta-classifier\n",
      "MLP5(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=5, out_features=2, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "True\n",
      "True\n",
      "clf_norm_scale_.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/holmes/lib/python3.8/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Loss: 0.649296\n",
      "\n",
      "Epoch 0 Test set: Average loss: 0.6664, Accuracy: 4583/7606 (60.2551%)\n",
      "\n",
      "laber 0 acc: 0.23\n",
      "laber 1 acc: 0.97\n",
      "Train Epoch: 10 Loss: 0.332008\n",
      "\n",
      "Epoch 10 Test set: Average loss: 0.3374, Accuracy: 6730/7606 (88.4828%)\n",
      "\n",
      "laber 0 acc: 0.90\n",
      "laber 1 acc: 0.87\n",
      "Train Epoch: 20 Loss: 0.247855\n",
      "\n",
      "Epoch 20 Test set: Average loss: 0.3125, Accuracy: 6823/7606 (89.7055%)\n",
      "\n",
      "laber 0 acc: 0.91\n",
      "laber 1 acc: 0.88\n",
      "Train Epoch: 30 Loss: 0.324523\n",
      "\n",
      "Epoch 30 Test set: Average loss: 0.3030, Accuracy: 6870/7606 (90.3234%)\n",
      "\n",
      "laber 0 acc: 0.92\n",
      "laber 1 acc: 0.88\n",
      "Train Epoch: 40 Loss: 0.210620\n",
      "\n",
      "Epoch 40 Test set: Average loss: 0.2930, Accuracy: 6900/7606 (90.7179%)\n",
      "\n",
      "laber 0 acc: 0.93\n",
      "laber 1 acc: 0.88\n",
      "Train Epoch: 50 Loss: 0.282198\n",
      "\n",
      "Epoch 50 Test set: Average loss: 0.2820, Accuracy: 6949/7606 (91.3621%)\n",
      "\n",
      "laber 0 acc: 0.93\n",
      "laber 1 acc: 0.89\n",
      "Train Epoch: 60 Loss: 0.282088\n",
      "\n",
      "Epoch 60 Test set: Average loss: 0.2757, Accuracy: 6969/7606 (91.6250%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.89\n",
      "Train Epoch: 70 Loss: 0.287942\n",
      "\n",
      "Epoch 70 Test set: Average loss: 0.2704, Accuracy: 7000/7606 (92.0326%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.90\n",
      "Train Epoch: 80 Loss: 0.277358\n",
      "\n",
      "Epoch 80 Test set: Average loss: 0.2653, Accuracy: 7023/7606 (92.3350%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.90\n",
      "Train Epoch: 90 Loss: 0.286262\n",
      "\n",
      "Epoch 90 Test set: Average loss: 0.2629, Accuracy: 7029/7606 (92.4139%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 100 Loss: 0.323063\n",
      "\n",
      "Epoch 100 Test set: Average loss: 0.2616, Accuracy: 7025/7606 (92.3613%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 110 Loss: 0.266200\n",
      "\n",
      "Epoch 110 Test set: Average loss: 0.2615, Accuracy: 7030/7606 (92.4270%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 120 Loss: 0.223822\n",
      "\n",
      "Epoch 120 Test set: Average loss: 0.2601, Accuracy: 7032/7606 (92.4533%)\n",
      "\n",
      "laber 0 acc: 0.95\n",
      "laber 1 acc: 0.90\n",
      "Train Epoch: 130 Loss: 0.229371\n",
      "\n",
      "Epoch 130 Test set: Average loss: 0.2604, Accuracy: 7033/7606 (92.4665%)\n",
      "\n",
      "laber 0 acc: 0.95\n",
      "laber 1 acc: 0.90\n",
      "Train Epoch: 140 Loss: 0.272583\n",
      "\n",
      "Epoch 140 Test set: Average loss: 0.2591, Accuracy: 7029/7606 (92.4139%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.90\n",
      "Train Epoch: 150 Loss: 0.286955\n",
      "\n",
      "Epoch 150 Test set: Average loss: 0.2591, Accuracy: 7044/7606 (92.6111%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 160 Loss: 0.307713\n",
      "\n",
      "Epoch 160 Test set: Average loss: 0.2589, Accuracy: 7040/7606 (92.5585%)\n",
      "\n",
      "laber 0 acc: 0.95\n",
      "laber 1 acc: 0.90\n",
      "Train Epoch: 170 Loss: 0.345493\n",
      "\n",
      "Epoch 170 Test set: Average loss: 0.2587, Accuracy: 7040/7606 (92.5585%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 180 Loss: 0.387652\n",
      "\n",
      "Epoch 180 Test set: Average loss: 0.2585, Accuracy: 7049/7606 (92.6768%)\n",
      "\n",
      "laber 0 acc: 0.95\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 190 Loss: 0.210981\n",
      "\n",
      "Epoch 190 Test set: Average loss: 0.2589, Accuracy: 7039/7606 (92.5454%)\n",
      "\n",
      "laber 0 acc: 0.95\n",
      "laber 1 acc: 0.90\n",
      "Train Epoch: 200 Loss: 0.207758\n",
      "\n",
      "Epoch 200 Test set: Average loss: 0.2591, Accuracy: 7050/7606 (92.6900%)\n",
      "\n",
      "laber 0 acc: 0.93\n",
      "laber 1 acc: 0.92\n",
      "Train Epoch: 210 Loss: 0.243926\n",
      "\n",
      "Epoch 210 Test set: Average loss: 0.2581, Accuracy: 7044/7606 (92.6111%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 220 Loss: 0.337719\n",
      "\n",
      "Epoch 220 Test set: Average loss: 0.2581, Accuracy: 7053/7606 (92.7294%)\n",
      "\n",
      "laber 0 acc: 0.95\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 230 Loss: 0.209770\n",
      "\n",
      "Epoch 230 Test set: Average loss: 0.2581, Accuracy: 7057/7606 (92.7820%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 240 Loss: 0.230736\n",
      "\n",
      "Epoch 240 Test set: Average loss: 0.2578, Accuracy: 7044/7606 (92.6111%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 250 Loss: 0.241271\n",
      "\n",
      "Epoch 250 Test set: Average loss: 0.2580, Accuracy: 7048/7606 (92.6637%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 260 Loss: 0.275976\n",
      "\n",
      "Epoch 260 Test set: Average loss: 0.2578, Accuracy: 7043/7606 (92.5979%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 270 Loss: 0.223506\n",
      "\n",
      "Epoch 270 Test set: Average loss: 0.2576, Accuracy: 7050/7606 (92.6900%)\n",
      "\n",
      "laber 0 acc: 0.95\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 280 Loss: 0.303195\n",
      "\n",
      "Epoch 280 Test set: Average loss: 0.2578, Accuracy: 7052/7606 (92.7163%)\n",
      "\n",
      "laber 0 acc: 0.95\n",
      "laber 1 acc: 0.91\n",
      "Train Epoch: 290 Loss: 0.282904\n",
      "\n",
      "Epoch 290 Test set: Average loss: 0.2573, Accuracy: 7053/7606 (92.7294%)\n",
      "\n",
      "laber 0 acc: 0.94\n",
      "laber 1 acc: 0.91\n",
      "Test on Victim best acc=0.928478\n",
      "Save clf at  ./ckpt/clf//clf_norm_scale_.pt\n",
      "Time cost: 161.22 sec\n"
     ]
    }
   ],
   "source": [
    "# train binary classifier\n",
    "print('train meta-classifier')\n",
    "clf = mlp.MLP5(len(outputs_trainset[0]), 2)\n",
    "clf = clf.to(device)\n",
    "print(clf)\n",
    "\n",
    "optimizer = optim.SGD(clf.parameters(), lr=0.005, weight_decay=5e-4, momentum=0.9)\n",
    "best_acc = 0\n",
    "\n",
    "norm_name = ''\n",
    "scale_name=''\n",
    "softmax_name=''\n",
    "ckpt_name='clf'\n",
    "\n",
    "\n",
    "if norm:\n",
    "    norm_name = 'norm'\n",
    "\n",
    "if scale:\n",
    "    scale_name = 'scale'\n",
    "\n",
    "if softmax:\n",
    "    softmax_name='softmax'\n",
    "\n",
    "ckpt_name=ckpt_name+'_'+norm_name+'_'+scale_name+'_'+softmax_name+'.pt'\n",
    "\n",
    "print(norm)\n",
    "print(scale)\n",
    "print(ckpt_name)\n",
    "\n",
    "isExists = os.path.exists(output_clf_dir)\n",
    "if not isExists:\n",
    "    os.makedirs(output_clf_dir)\n",
    "\n",
    "for epoch in range(mlp_epoch):\n",
    "    with autocast(device_type='cuda'):\n",
    "        mlp.train(clf, outputs_trainset, outputs_trainlabel, epoch, optimizer, device)\n",
    "        acc = mlp.test(clf, outputs_trainset, outputs_trainlabel, device, epoch)\n",
    "        best_mlp_path = output_clf_dir+'/'+ckpt_name\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(clf.state_dict(), best_mlp_path)\n",
    "\n",
    "print(\"Test on Victim best acc=%.6f\" % best_acc)\n",
    "print('Save clf at ', best_mlp_path)\n",
    "\n",
    "print('Time cost: {} sec'.format(round(time.time() - starttime, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5c1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import hmean\n",
    "from scipy import stats\n",
    "\n",
    "def get_p_value(arrA, arrB, alternative='greater'):\n",
    "    a = np.array(arrA)\n",
    "    b = np.array(arrB)\n",
    "    t, p = stats.ttest_ind(a, b, alternative=alternative, equal_var=False)\n",
    "    return p\n",
    "\n",
    "\n",
    "def mult_test(prob_f, prob_nf, seed, m, mult_num=40, alternative='greater'):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    p_list = []\n",
    "    mu_list = []\n",
    "    np.random.seed(seed)\n",
    "    for t in range(mult_num):\n",
    "        sample_num = m\n",
    "        sample_list = [i for i in range(len(prob_f))]\n",
    "        sample_list = random.sample(sample_list, sample_num)\n",
    "\n",
    "        subprob_f = prob_f[sample_list]\n",
    "        subprob_nf = prob_nf[sample_list]\n",
    "        p_val = get_p_value(subprob_f, subprob_nf, alternative)\n",
    "        p_list.append(p_val)\n",
    "        mu_list.append(np.mean(subprob_f) - np.mean(subprob_nf))\n",
    "    return p_list, mu_list\n",
    "\n",
    "\n",
    "def get_prob_pair(clf, sus_list, device):\n",
    "    prob_sus = []\n",
    "    for i in range(len(sus_list)):\n",
    "        sus_o = torch.from_numpy(sus_list[i])\n",
    "        sus_o = sus_o.to(device)\n",
    "\n",
    "        out_sus = clf(sus_o)\n",
    "\n",
    "        prob_sus.append(out_sus.cpu().detach().numpy())\n",
    "\n",
    "    return prob_sus\n",
    "\n",
    "def verification_test(sus_output, poison_output, steal_name='None'):\n",
    "    sus_diff = sus_output - poison_output\n",
    "\n",
    "    sus_diff = get_prob_pair(clf, sus_diff, device)\n",
    "\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    softmax_sus_diff = softmax(torch.from_numpy(np.array(sus_diff)))\n",
    "\n",
    "    # (benign, 0) and (vict, 1)\n",
    "    sus_diff_stolen = softmax_sus_diff[:, 1]\n",
    "    sus_diff_benign = softmax_sus_diff[:, 0]\n",
    "\n",
    "\n",
    "    seed = 100\n",
    "    m = 100\n",
    "\n",
    "    p_list, mu_list = mult_test(sus_diff_stolen.numpy(), sus_diff_benign.numpy(), seed=seed, m=m, mult_num=40, alternative='greater')\n",
    "    print('{}:  p-val: {} mu: {}'.format(steal_name, hmean(p_list), np.mean(mu_list)))\n",
    "\n",
    "def verification_test_2(sus_output, poison_output, seed, m, steal_name='None'):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    if_norm = False\n",
    "    if_scale = False\n",
    "    if_softmax = False\n",
    "\n",
    "    if_load_norm=False\n",
    "    if_load_scale=False\n",
    "\n",
    "    if 'norm' in ckpt_name:\n",
    "        if_norm = True\n",
    "    if 'scale' in ckpt_name:\n",
    "        if_scale = True\n",
    "    \n",
    "    sus_diff = sus_output - poison_output\n",
    "\n",
    "\n",
    "    if if_norm:\n",
    "        normalizer = Normalizer(norm='l2')\n",
    "        sus_diff = normalizer.transform(sus_diff)\n",
    "\n",
    "    if if_scale:\n",
    "        scaler = RobustScaler()\n",
    "        sus_diff = scaler.fit_transform(sus_diff)\n",
    "\n",
    "    sus_diff = get_prob_pair(clf, sus_diff, device)\n",
    "\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    softmax_sus_diff = softmax(torch.from_numpy(np.array(sus_diff)))\n",
    "\n",
    "    # (benign, 0) and (vict, 1)\n",
    "    sus_diff_stolen = softmax_sus_diff[:, 1]\n",
    "    sus_diff_benign = softmax_sus_diff[:, 0]\n",
    "\n",
    "    p_list, mu_list = mult_test(sus_diff_stolen.numpy(), sus_diff_benign.numpy(), seed=seed, m=m, mult_num=40, alternative='greater')\n",
    "    print('{}:  p-val: {} mu: {}'.format(steal_name, hmean(p_list), np.mean(mu_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561bd3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct-copy:  p-val: 1.9865974516909697e-18 mu: 0.240617036819458\n",
      "\n",
      "Fine-tuning:  p-val: 5.957258553445839e-16 mu: 0.2647797167301178\n",
      "\n",
      "Data Distill:  p-val: 4.526801730669385e-09 mu: 0.1560070812702179\n",
      "\n",
      "Data-free Distill:  p-val: 4.666670116955149e-11 mu: 0.1875937581062317\n",
      "\n",
      "Hard Distill:  p-val: 1.0684542526591873e-05 mu: 0.06177513673901558\n",
      "\n",
      "Soft Distill:  p-val: 0.0030668361297120695 mu: 0.054538749158382416\n",
      "\n",
      "Independent:  p-val: 0.48637868587127725 mu: -0.08097676932811737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verification\n",
    "\n",
    "seed = 300\n",
    "m=100\n",
    "\n",
    "# Soft Distill\n",
    "\n",
    "outputset_dict = {\n",
    "    'Direct-copy':'./outputset-ckpt/stolen/Direct-copy/benign_sample_train_logits_list.npy',\n",
    "    'Fine-tuning':'./outputset-ckpt/stolen/Fine-tuning/benign_sample_train_logits_list.npy',\n",
    "    'Data Distill': './outputset-ckpt/stolen/Data-distill/benign_sample_train_logits_list.npy',\n",
    "    'Data-free Distill': './outputset-ckpt/stolen/Data-free-distill/benign_sample_train_logits_list.npy',\n",
    "    'Hard Distill': './outputset-ckpt/stolen/Hard-distill/benign_sample_train_logits_list.npy',\n",
    "    'Soft Distill': './outputset-ckpt/stolen/Soft-distill/benign_sample_train_logits_list.npy',\n",
    "    'Independent': './outputset-ckpt/stolen/Independent/benign_sample_train_logits_list.npy',\n",
    "}\n",
    "\n",
    "\n",
    "for item in outputset_dict.keys():\n",
    "    steal_name = item\n",
    "    sus_output_dir = outputset_dict[steal_name]\n",
    "    sus_output = np.load(sus_output_dir)\n",
    "    verification_test_2(sus_output, poison_output, seed, m, steal_name)\n",
    "    print()\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holmes",
   "language": "python",
   "name": "holmes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
